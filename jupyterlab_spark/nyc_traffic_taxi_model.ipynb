{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Referências\n",
    "- https://learn.microsoft.com/en-us/fabric/data-science/fabric-sparkml-tutorial"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Obtenção dos dados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.opendatasets import NycTlcYellow\n",
    "import pandas as pd\n",
    "from dateutil import parser\n",
    "\n",
    "end_date = parser.parse('2018-05-05 00:00:00')\n",
    "start_date = parser.parse('2018-05-01 00:00:00')\n",
    "nyc_tlc = NycTlcYellow(start_date=start_date, end_date=end_date)\n",
    "nyc_tlc_pd = nyc_tlc.to_pandas_dataframe()\n",
    "\n",
    "# Sample 1: Using a specific seed\n",
    "nyc_tlc_pandas_sample_1 = nyc_tlc_pd.sample(\n",
    "    frac=0.001,\n",
    "    replace=True,\n",
    "    random_state=1234\n",
    ")\n",
    "\n",
    "nyc_tlc_pd.to_csv(\"nyc_tlc_train_test.csv\")\n",
    "\n",
    "# Sample 2: Using a different seed\n",
    "nyc_tlc_pandas_sample_2 = nyc_tlc_pd.sample(\n",
    "    frac=0.001,\n",
    "    replace=True,\n",
    "    random_state=5678\n",
    ")\n",
    "\n",
    "nyc_tlc_pd.to_csv(\"nyc_tlc_predict.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Import the libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "from pyspark.sql.functions import unix_timestamp, date_format, col, when\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml import PipelineModel\n",
    "from pyspark.ml.feature import RFormula\n",
    "from pyspark.ml.feature import OneHotEncoder, StringIndexer, VectorIndexer\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "from pyspark.mllib.evaluation import BinaryClassificationMetrics\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
    "from pyspark.sql import SparkSession\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Criação da conexão Spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a SparkSession with specific configurations\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Spark Application\") \\\n",
    "    .master(\"local[*]\") \\\n",
    "    .config(\"spark.driver.memory\", \"2g\") \\\n",
    "    .config(\"spark.driver.cores\", \"2\") \\\n",
    "    .config(\"spark.executor.memory\", \"2g\") \\\n",
    "    .config(\"spark.executor.cores\", \"2\") \\\n",
    "    .config(\"spark.dynamicAllocation.enabled\", \"true\") \\\n",
    "    .config(\"spark.dynamicAllocation.minExecutors\", \"1\") \\\n",
    "    .config(\"spark.dynamicAllocation.maxExecutors\", \"4\") \\\n",
    "    .config(\"spark.python.worker.timeout\", \"600\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Construct input pyspark.dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_url = \"/home/jovyan/work/nyc_tlc_train_test.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nyc_tlc_pandas_sampled = pd.read_csv(sample_url)\n",
    "\n",
    "nyc_tlc_pandas_sampled = (\n",
    "    spark.createDataFrame(nyc_tlc_pandas_sampled)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Prepare the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "taxi_df = (\n",
    "    nyc_tlc_pandas_sampled\n",
    "    .select(\n",
    "        'totalAmount',\n",
    "        'fareAmount',\n",
    "        'tipAmount',\n",
    "        'paymentType',\n",
    "        'rateCodeId',\n",
    "        'passengerCount',\n",
    "        'tripDistance',\n",
    "        'tpepPickupDateTime',\n",
    "        'tpepDropoffDateTime',\n",
    "        date_format('tpepPickupDateTime', 'hh').alias('pickupHour'),\n",
    "        date_format('tpepPickupDateTime', 'EEEE').alias('weekdayString'),\n",
    "        (unix_timestamp(col('tpepDropoffDateTime')) - unix_timestamp(col('tpepPickupDateTime'))).alias('tripTimeSecs'),\n",
    "        (when(col('tipAmount') > 0, 1).otherwise(0)).alias('tipped')\n",
    "    )\n",
    "    .filter(\n",
    "        (col('passengerCount') > 0) &\n",
    "        (col('passengerCount') < 8) &\n",
    "        (col('tipAmount') >= 0) &\n",
    "        (col('tipAmount') <= 25) &\n",
    "        (col('fareAmount') >= 1) &\n",
    "        (col('fareAmount') <= 250) &\n",
    "        (col('tipAmount') < col('fareAmount')) &\n",
    "        (col('tripDistance') > 0) &\n",
    "        (col('tripDistance') <= 100) &\n",
    "        (col('rateCodeId') <= 5) &\n",
    "        (col('paymentType').isin(\"1\", \"2\"))\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "taxi_featurised_df = (\n",
    "    taxi_df\n",
    "    .select(\n",
    "        'totalAmount',\n",
    "        'fareAmount',\n",
    "        'tipAmount',\n",
    "        'paymentType',\n",
    "        'passengerCount',\n",
    "        'tripDistance',\n",
    "        'weekdayString',\n",
    "        'pickupHour',\n",
    "        'tripTimeSecs',\n",
    "        'tipped',\n",
    "        when((col('pickupHour') <= 6) | (col('pickupHour') >= 20), \"Night\")\n",
    "        .when((col('pickupHour') >= 7) & (col('pickupHour') <= 10), \"AMRush\")\n",
    "        .when((col('pickupHour') >= 11) & (col('pickupHour') <= 15), \"Afternoon\")\n",
    "        .when((col('pickupHour') >= 16) & (col('pickupHour') <= 19), \"PMRush\")\n",
    "        .otherwise(\"Unknown\").alias('trafficTimeBins')  # Changed 0 to \"Unknown\" for consistency\n",
    "    )\n",
    "    .filter(\n",
    "        (col('tripTimeSecs') >= 30) &\n",
    "        (col('tripTimeSecs') <= 7200)\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create a LR Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import StringIndexer, OneHotEncoder\n",
    "from pyspark.ml import Pipeline\n",
    "\n",
    "# Create StringIndexer and OneHotEncoder instances for categorical features\n",
    "sI1 = StringIndexer(\n",
    "    inputCol=\"trafficTimeBins\",\n",
    "    outputCol=\"trafficTimeBinsIndex\"\n",
    ")\n",
    "\n",
    "en1 = OneHotEncoder(\n",
    "    dropLast=False,\n",
    "    inputCol=\"trafficTimeBinsIndex\",\n",
    "    outputCol=\"trafficTimeBinsVec\"\n",
    ")\n",
    "\n",
    "sI2 = StringIndexer(\n",
    "    inputCol=\"weekdayString\",\n",
    "    outputCol=\"weekdayIndex\"\n",
    ")\n",
    "\n",
    "en2 = OneHotEncoder(\n",
    "    dropLast=False,\n",
    "    inputCol=\"weekdayIndex\",\n",
    "    outputCol=\"weekdayVec\"\n",
    ")\n",
    "\n",
    "# Create and fit a Pipeline to apply the transformations\n",
    "pipeline = Pipeline(stages=[sI1, en1, sI2, en2])\n",
    "\n",
    "# Transform the DataFrame with the Pipeline\n",
    "encoded_final_df = pipeline.fit(taxi_featurised_df).transform(taxi_featurised_df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Starts the MLFLOW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mlflow\n",
    "\n",
    "mlflow.set_experiment(f'nyc_traffic_taxi')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "now = datetime.now()\n",
    "\n",
    "current_time = now.strftime(\"%d/%m/%Y - %H:%M:%S\")\n",
    "current_time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train and test the LR Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the fractions for splitting the data\n",
    "training_fraction = 0.7\n",
    "testing_fraction = 1 - training_fraction\n",
    "seed = 1234\n",
    "\n",
    "# Split the DataFrame into training and testing DataFrames\n",
    "train_data_df, test_data_df = encoded_final_df.randomSplit(\n",
    "    weights=[training_fraction, testing_fraction],\n",
    "    seed=seed\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.classification import LogisticRegression\n",
    "from pyspark.ml.feature import RFormula\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.mllib.evaluation import BinaryClassificationMetrics\n",
    "from datetime import datetime\n",
    "\n",
    "# Define the formula for the model\n",
    "classFormula = RFormula(\n",
    "    formula=\"tipped ~ pickupHour + weekdayVec + passengerCount + tripTimeSecs + tripDistance + fareAmount + paymentType + trafficTimeBinsVec\"\n",
    ")\n",
    "\n",
    "for i in range(1, 11):\n",
    "\n",
    "    print(\"LR TRAIN AND TEST: \", i)\n",
    "    \n",
    "    reg_param = 0.1 * i\n",
    "\n",
    "    #-----MLFLOW-----\n",
    "    run_name = f'lr_model_train_{i}'\n",
    "\n",
    "    runs = {\n",
    "        \"run_name\": run_name\n",
    "    }\n",
    "\n",
    "    # Start run\n",
    "    mlflow.start_run(run_name=run_name)\n",
    "\n",
    "    run = mlflow.active_run()\n",
    "    \n",
    "    run_id = run.info.run_id\n",
    "\n",
    "    parametros = {\n",
    "        \"reg_param\": reg_param,\n",
    "    }\n",
    "    #----------------\n",
    "    \n",
    "    # Create a new Logistic Regression object for the model\n",
    "    logReg = LogisticRegression(\n",
    "        maxIter=10,\n",
    "        regParam=reg_param,\n",
    "        labelCol='tipped'\n",
    "    )\n",
    "    \n",
    "    # Create a Pipeline with the formula and logistic regression stages\n",
    "    pipeline = Pipeline(stages=[classFormula, logReg])\n",
    "    \n",
    "    # Train the logistic regression model\n",
    "    lrModel = pipeline.fit(train_data_df)\n",
    "    \n",
    "    # Predict tip 1/0 (yes/no) on the test dataset and evaluate using area under ROC\n",
    "    predictions = lrModel.transform(test_data_df)\n",
    "    \n",
    "    # Convert predictions to RDD and compute metrics\n",
    "    prediction_and_labels = predictions.select(\"label\", \"prediction\").rdd\n",
    "    metrics = BinaryClassificationMetrics(prediction_and_labels)\n",
    "\n",
    "    #-----MLFLOW-----\n",
    "    \n",
    "    metricas = {\n",
    "        \"area_under_roc\": metrics.areaUnderROC\n",
    "    }\n",
    "\n",
    "    print(\"area_under_roc: \", metrics.areaUnderROC)\n",
    "    \n",
    "    mlflow.set_tag(\"data\", current_time)\n",
    "    mlflow.log_params(parametros)\n",
    "    mlflow.log_metrics(metricas)\n",
    "    mlflow.log_artifact(sample_url)\n",
    "\n",
    "    mlflow.spark.log_model(spark_model=lrModel, artifact_path=\"lr_model\")\n",
    "    \n",
    "    mlflow.end_run()\n",
    "\n",
    "    print(\"-\"*50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Close the Spark Connection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
